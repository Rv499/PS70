<!DOCTYPE html>
<html lang="en">

<title>PS70: Intro to Digital Fabrication </title>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" rel="stylesheet">
<link href="../style.css" rel="stylesheet">


<nav class="navbar navbar-expand-sm navbar-dark" style="background-color: #130F1D; color: #EEE7E8;">
  <div style="align-items: center; justify-content: center;" class="container-fluid">
    <div class="flexrow">
      <h2 class="nav-title">PS70 // Intro to Digital Fabrication // Spring 24</h2>
    </div>
    <div class="navbar-nav">
      <h4><a class="nav-link" href="../index.html">Home</a></h4>
      <h4><a class="nav-link" href="../about.html">About</a></h4>
    </div>
  </div>
</nav>

<body>
<xmp style="display:none;">
<div class="textcontainer">
<p class="margin"></p>

<h3>Final Project: Heart Rate Driven Journal Prompt Wearable </h3>



<p class="margin"></p>
<p class="margin"></p>
<div class="flexrow">
  <a id="btn" href="finalproject.zip" download>Download the files used for my final project!
  </a>
</div>
<p class="margin"></p>

<h4>Phase 1: Brainstorming</h4>

<p class="margin"></p>
This project has had some changes since I have started the course. My orignal idea was to use EEG sensors to understand the emotional state of a person. These emotional states will be used to derive journal prompts. The follow on prompts would change based on the changed emotional state of the person. Unfortunately, this idea would be too complex from a data analysis standpoint. So I decided to go with a more simpler project. This new project would be a wearble device like a braclet that would were the device software analyzes the heart rate data to determine the user’s emotional or stress level (e.g., relaxed, anxious, excited). Based on this analysis, it generates journal prompts tailored to encourage reflection on the user’s current state. For example, a higher heart rates might trigger prompts related to managing stress or excitement, while lower rates could lead to prompts focused on gratitude or calmness.

<p class="margin"></p>
**Supplies**

- PPG Device
  - ~~MAX30100~~ or MAX30102
- ESP32 Board
- 3D Molded housing for ESP32 and PPG device
- Some type of small lcd interface to view prompts
- Battery

<p class="margin"></p>

**Software**: 


- Algorithm for interpreting heart rate data. MAX30102 library
- a database of journal prompts categorized by emotional state

<p class="margin"></p>


<p class="margin"></p>



<p class="margin"></p>
<div class="flexrow">
  <img src="MAX30102.jpeg" alt="a MAX30102 Board">
  <img src="Potential Diagram.jpeg" alt="a screenshot of possible 3D Model">
</div>
<p class="caption">Some early diagrams of device components.</p>


<p class="margin"></p>
<p class="margin"></p>
<h4>Phase 2: Prototyping</h4>

<p class="margin"></p>
The prototyping phase of this started when we learned about input devices. This helped me figure out the type of sensor I would want to use for the device. I narrowed it down to two sensors. The MAX3010 or the MAX30102. I decided to go with the MAX30102 due to the better accuracy of the sensor and a recent study that came out about the sensor located here: https://www.mdpi.com/2673-4591/16/1/9 .The sensor shape determines the foundation for the size and shape of the device.

<p class="margin"></p>
<p class="margin"></p>
<h4>Phase 3: Final Planning</h4>

<p class="margin"></p>
**Coming soon**

<p class="margin"></p>
<p class="margin"></p>
<h4>Phase 4: Assembly</h4>
Hardware Components
ESP32 Microcontroller: Chosen for its WiFi capabilities and compatibility with a variety of sensors.
Pulse Sensor: A simple heart rate sensor used to monitor the heart rate in real-time.
OLED Display: Used to provide immediate feedback and display the journal prompts generated based on the heart rate data.
Connecting Wires: For establishing connections between the ESP32, the pulse sensor, and the OLED display.
Software Components
Arduino IDE: Used for programming the ESP32.
Adafruit SSD1306 Library: For controlling the OLED display.
PulseSensor Playground Library: For interfacing with the pulse sensor.
WiFi and ChatGPT Libraries: To connect to the internet and interact with the ChatGPT API.
<p class="margin"></p>
Development Process and Challenges
Initial Assembly and Sensor Integration

The initial phase involved connecting the pulse sensor to the ESP32 and displaying the heart rate data on the OLED screen. This straightforward setup served as the foundation for the more complex features that would be added later.

Venturing into EEG

Mid-project, there was an attempt to incorporate EEG sensors to provide a more comprehensive view of the user's emotional state by reading brain waves. However, integrating EEG sensors proved to be significantly more complex and required additional time for calibration and interpretation of the data. Due to these complexities, the project scope was refocused back to using just the heart rate sensor.

Pin Configuration Challenges

A major issue arose when trying to read the heart rate data on the ESP32, which initially did not work as smoothly as it had on the Arduino. After some investigation, it was discovered that not all pins on the ESP32 are capable of reading analog input. Switching the sensor input to a correct analog-capable pin resolved this issue.

Integration with ChatGPT

Connecting the ESP32 to the ChatGPT API initially resulted in errors, which were later identified as issues related to API access permissions. It became clear that a paid API subscription was necessary to use ChatGPT effectively, after which the integration proceeded without further issues.

Display Issues

The OLED display initially did not function, which was eventually traced back to a hardware fault. Replacing the broken monitor resolved this problem, allowing the display of the heart rate and journal prompts. Additionally, there was an issue with the amount of text being displayed. The feedback from ChatGPT often exceeded the display capacity of the OLED screen, necessitating a modification to the code to truncate responses to 100 characters.

Conclusion
This project demonstrated the feasibility of combining physiological data with AI-generated content to create a responsive and interactive device. While there were several challenges, such as hardware limitations and the need for a paid API, each was overcome through systematic troubleshooting and adjustments.


</div>
</xmp>
</body>

<script src="../strapdown.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" ></script>

</html>